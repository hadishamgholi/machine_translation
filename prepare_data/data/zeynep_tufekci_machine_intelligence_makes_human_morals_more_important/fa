
خب، من اولین شغلم را به عنوان یک 
برنامه نویس کامپیوتر شروع کردم
در اولین سال کالجم
در واقع، به عنوان یک نوجوان


درست پس از شروع به کار در
یک شزکت به عنوان برنامه‌نویس کامپیوتر ،
یک مدیر که در همان جا کار می‌کرد
نزد من آمد،
و با صدای آرام به من گفت:
"او می تونه بگه اگه من دروغ بگم؟"
در اتاق هیچ کسی وجود نداشت.


"کسی میتونه بگه اگه تو دروغ میگی؟
و چرا ما در حال پچ پچ کردن هستیم؟"


مدیر کامپیوتر در اتاق را نشان داد.
"او می‌تونه بگه اگه من دروغ بگم؟"
خب، آن مدیر رابطه نامشروع با منشی‌اش داشت.


(خنده حاضرین)


و من هم هنوز یک نوجوان بودم
سپس من با صدای بلندی گفتم:
"بله، کامپیوتر می تونه بگه 
اگه تو دروغ بگی"


(خنده حاضرین)


خب، من خندیدم، ولی در واقع،
خنده به خودم
امروزه، سامانه‌های محاسباتی وجود دارد
که حالت احساسی و حتی دروغ رو 
از طریق تحلیل صورت انسان
می تونه بفهمه
تبلیغ کننده‌ها و حتی دولت‌ها خیلی
جالبند.


من یه برنامه نویس کامپیوتر
شده بودم
زیرا من یکی از آن بچه‌های دیوانه 
ریاضی و علم بودم
اما در مسیر زندگی من در مورد
سلاح‌های هسته‌ای چیزهایی یادگرفتم‌،
و واقعا در مورد اخلاق علمی نگران شدم.
من وحشت زده بودم
به هر حال، به دلیل موقعیت خانواده‌ام
نیاز داشتم تا
در اسرع وقت- کارم را شروع کنم
خُب با خودم فکر کردم
بگذار تا یک رشته تکنیکی را بردارم
که شغل راحتی باشه
و من با هیچ پرسش سخت اخلاقی 
مواجه نشوم؟
پس کامپیوترها را انتخاب کردم.


(خنده حاضرین)


خب، ها، ها، ها!
همه خنده‌ها برای من هستند.
امروزه، دانشمندان کامپیوتر
در حال ساخت یک سیستم عامل هستند
که آنچه یک میلیارد آدم
هر روز می‌بینند روکنترل می‌کنه
آنها خودروهایی را ساختند که می‌توانند
تصمیم بگیرند که چه کسی زیر بگیرند.
آن ها حتی در حال ساخت ماشین‌هایی هستند،
تسلیحاتی
که ممکنه آدم ها رو در جنگ بکشه.
همه اینها سقوط اخلاق‌ است.


هوشمندی ماشین اینجاست.
ما در حال استفاده از محاسباتی
هستیم که همه تصمیمات ما را مرتب می‌کنه،
همچنین نوع‌های مختلف تصمیم‌ها را مرتب می‌کند.
ما سوالاتی را می پرسیم هستیم که 
هیچ جواب معین درستی ندارند،
آنها ذهنی هستند
و بدون جواب معین 
و پر محتوا هستند.


ما پرسش‌هایی شبیه اینها را می‌پرسیم:
«شرکت کی باید استخدام کند؟»
«بکدام بروزرسانی از کدام دوست 
باید نشان داده شود؟»
«کدام متهم بیشتر شبیه خلافکارهاست؟»
«کدام بخش خبرها یا فیلم باید به مردم 
توصیه بشه؟»


ببیند، بله، ما مدت‌هاست است که در حال 
استفاده از کامپیوترها هستیم
اما این بار فرق داره.
این یک چرخش تاریخی است،
زیرا ما نمی‌توانیم محاسبات را 
برای تصمیم‌های ذهنی نگه داریم
مانند روش محاسباتی
برای پرواز هواپیما، ساخت پل‌ها
و به ماه رفتن.
آیا هواپیماها امن هستند؟
آیا این پل فرو می‌ریزد؟
این چنین است، ما منصفانه و براساس معیارهای
روشن توافق کردیم
و ما قوانین طبیعت را برای راهنمایی داریم
ما چیزی شبیه مجری ها و معیارها 
برای تصمیم گیری
درکارهای انسان آشفته نداریم.


برای انجام کارهای پیچیده‌تر،
تا نرم افزارهای ما بیشتر قدرتمند می‌شوند،
اما این می تواند کمتر شفاف 
و بیشتر پیچیده باشد.
اخیرا، در دهه گذشته
الگوریتم های پیچیده
با گام‌های بلندی ساخته شده‌اند.
آنها می‌توانند صورت انسان را
بازشناسایی کنند.
آنها می‌توانند دست خط را تشخیص بدهند.
آنها می‌توانند تقلب در کارت اعتباری را
کشف کنند
و اسپم‌ها را مسدود کنند
و آنها می‌توانند زبان‌ها را ترجمه کنند،
می توانند تومورها را در تصاویر پزشکی
کشف کنند.
آنها می‌توانند در بازیهای
شطرنج و گو از آدمها ببرند.


بیشتر این پیشرفت‌ها از روشی به نام 
"یادگیری ماشین" آمده‌اند.
یادگیری ماشین با برنامه نویسی سنتی 
متفاوت هست،
که به کامپیوتر جزئیات دقیق
دستورات پر زحمت را میدهید.
این بیشتر شبیه اینه که شما یک سیستم
گرفته‌اید و اطلاعات زیادی به آن می‌خورانید
شامل اطلاعات بدون ساختار،
مانند اطلاعاتی که ما در زندگی دیجیتال خود
تولید می‌کنیم.
و سیستمی که بوسیله گردش در بین اطلاعات
یاد میگیرد.
و همچنین بحرانی
آن سیستم‌هایی که زیر یک پاسخ سیگنال
منطقی عمل نمی کنند
آن ها یک پاسخ ساده تولید نمی کنند
این ها بیشتر "احتمال" هستند
"این یک احتمالی است که بیشتر شبیه
آنچه شما دنبال آن هستید "


حالا، بالاتر این است که: این شیوه
واقعا قدرتمند است.
رییس سیستم‌های هوش مصنوعی (AI) گوگل
این را نام گذاری کرد:
"اثر غیر منطقی اطلاعات"
قسمت بدترش این است که:
ما واقعا نمی‌فهمیم 
سیستم چه یاد می‌گیرد.
در حقیقت این قدرت آن است.
این کمتر شبیهِ دادنِ دستورالعمل
به کامپیوتر است،
این بیشتر شبیه یاد دادن به یک
توله ماشین زندهاست
ما واقعا نمی‌فهمیم و کنترل نمی‌کنیم.
خب این مشکل ماست.
این یک مشکل است وقتی که سیستم هوش مصنوعی
چیزها را اشتباه یاد میگیرد.
این همچنین یک مشکل است وقتی که 
این چیزها را درست یاد میگیرد،
زیرا ما حتی نمی دانیم کدام به کدام است
وقتی که این یک مشکل درونی است.
ما نمی‌دانیم در حال فکر کردن 
به چه چیزی است


یک الگوریتم استخدام را فرض کنید—
یک سیستمی که مردم را با استفاده از 
سیستم یادگیری ماشین استخدام می کند.
مانند یک سیستمی که بر اساس اطلاعات
کارمندان قبلی آموزش دیده شده است
و دستور دارد که پیدا کند و استخدام کند
مردمی که بهروری بالایی در شرکت
دارند.
به نظر خوب میاد.
من یک بار در کنفرانسی حضور داشتم
که مدیران منابع انسانی و مدیران اجرایی
دور هم جمع شده بودند،
افراد رده بالای شرکت‌ها
که از این سیستم‌های برای استخدام
استفاده می کردند.
آنها خیلی هیجان زده بودند
آنها فکر می کردند که این استخدام را بیشتر هدفمند 
و کمتر مغروضانه خواهند بود،
و به خانم ها و اقلیت 
یک شانس بهتری میدهد
بر خلاف غرض‌ورزی مدیران منابع انسانی


ببینید—
استخدام افراد غرض ورزانه است.
من میدانم.
منظورم اینه، در اولین شغل من به عنوان
برنامه نویس
مدیر بخش من گاهی اوقات
در اول صبح یا آخر عصر پیش من می‌آمد
و میگفت: "زینب بیا بریم ناهار"
من به خاطر زمان‌های عجیب گیج می‌شدم
الان ساعت ۴ است، ناهار؟
من شکست می‌خوردم
من همیشه برای ناهار مجانی می‌رفتم
بعدها فهمیدم که چه اتفاقی می‌افتاد
مدیران بالایی من در انتخابشان برای استخدام 

یک دختر نوجوان که کفش کتانی
و جین در محل کار می‌پوشید
برای انجام یک کار جدی
اشتباه نکرده بودند.
من خوب کار می‌کردم،
اما به نظرمیآمد که من مناسب نیستم
سن و جنسیتم نیز اشتباه بود.


خُب نادیده گرفتن شدن جنسیت و نژاد من
قطعا چیزی خوبی برای من بود.
اما با این سیستم ها
بغرنج‌تر و پیچیده‌تر شده و دلیلیش اینجاست:
اخیرا، سیستم‌های محاسبه‌گر می‌توانند 
به همه چیزهای شما
از طریق خرده اطلاعات
دیجیتالی شما پی‌ببرند،
حتی اگر شما آن چیزها را فاش نکرده باشید.
آنها به گرایش‌های جنسی‌تان ،
ویژگی‌های شخصی‌تان،
دانسته‌های سیاسی‌تان پی‌ببرند.
آنها قدرت پیش بینی با صحت بالایی را دارند.
به یاد داشته باشید، برای چیزهایی که شما
حتی آنها را فاش نکرده‌اید
نتیجه گیری‌ و استنتاج است.


من یک دوستی دارم که سیستم های 
محاسبه گری را توسعه می‌دهد
تا شانس افسردگی بالینی یا بعد از وضع حمل
را پیش بینی کند
با استفاده از اطلاعات رسانه های اجتماعی
نتیجه ها هیجان انگیز هستند.
سیستمش احتمال افسردگی را
می‌توان
ماه‌های قبل از شروع علائم 
بیماری را پیش‌بینی کند—
ماه‌های قبل.
هیچ علامتی از بیماری نیست، 
ولی پیش بینی می‌شود.
او امیدوار است بزودی این را برای مداخلات
(روانشناسی) استفاده کند . عالیه.
اما حالا این در فضای استخدام قرار دهید.


بنابراین در این کنفرانس منبع مدیران انسانی
من به یک مدیر سطح بالا 
در یک شرکت بزرگ نزدیک شدم
و به او گفتم: "ببین، چه میشد اگر من برای 
تو فردی ناشناخته می‌شدم؟"
آیا سیستم تو در حال حذف مردم 
با احتمال بالای افسردگی در آینده، است ؟
آن ها الان افسرده نیستند، فقط شاید در 
آینده به احتمال زیاد دچار افسردگی شوند.
یا اگر زنانی که احتمال دارد در یکی 
دو سال آینده باردار شوند
ولی الان حامله نیستد
را کنار گذارده شوند؟
اگر این افراد پرخاشکر را استخدام شوند
زیرا آن فرهنگ محیط کاریت است
تو با استفاده از نگاه کردن به
تقسیم بندی جنسبت نمی توانی بگویی
آن ها ممکنه متعادل باشند.
و چون این یادگیری ماشین است و 
برنامه نویسی سنتی نیست
هیچ متغیری وجود ندارد که 
"بیشترین خطر افسردگی " نام گذاری شود
"بیشترین خطر حاملگی"
"مقیاس پرخاشگری مردان"
نه تنها نمی دانست 
چگونه سیستم شما، انتخاب می کنه
شما حتی نمی دانی که
در کجا جستجو می‌کند
این یک جعبه سیاه است.
این قدرت پیش گویی دارد
اما شما این را نمی‌فهمی


من پرسسیدم"حفاظت چیست؟"
« آیا باید مطمئن شوی که جعبه سیاه تان
کار مشکوکی انجام نمی‌دهد؟»
او به من به نگاه کرد مثل اینکه من
پا روی دُم ده تا توله گذاشتم!



(خنده حاضرین)


او به من خیره شد و گفت:
«نمی‌خوام کلمه دیگری در این باره بشنوم»
و او برگشت و قدم زنان دور شد.
به خاطر داشته باشید او بی‌ادب نبود.
کاملا روشن بود با نگاهش می‌گفت: نمی‌دونم،
این مشکل من نیست، برو.


(خنده حاضرین)


نگاه کنید، یک سیستم ممکن است
حتی کمتر جانبدارانه باشد
تا مدیران انسانی در همان زمینه.
و این میتونه یک حس مالی ایجاد کنه
اما این می‌تونه منجر بشه
به یک یکنواختی اما یواشکی بستن
بازار کار مردم
که با ریسک بالای افسردگی همراه هستند.
آیا این نوع اجتماعی است 
که ما می‌خواهیم بسازیم؟
بدون حتی دانستن اینکه ما این را
انجام دادیم
زیرا ما ساختن تصمیم را تبدیل کردیم
به ماشین که ما سرانجامش را نمی‌فهمیم


و مسئله دیگر این است:
این سیستم ها اغلب روی اطلاعات تولید شده
توسط کارهای ما آموزش داده می‌شوند،
آثار به جای مانده از انسان.
خب، آنها فقط می‌توانند
تمایلات ما را منعکس کنند،
و این سیستم‌های می‌توانند
تمایلات ما را انتخاب کنند
و آن را تقویت کرده
و آن را دوباره به ما نشان دهند،
در حالی که به خودمان می‌گویم،
«ما فقط در حال بررسی هستیم.»


محققان در شرکت گوگل دریافتند،
زنان نسبت به مردان احتمال کمتری دارد که
برای مشاغل با حقوق بالاتر قدام کنند.
و نام‌های آفریقایی-آمریکایی را که جستجو کنید
احتمال بیشتر دارد که پیشینه جرم نشان دهد،
حتی وقتی که واقعا جرمی وجود ندارد.
مانند تمایلات پنهان و الگوریتم جعبه سیاه
که گاهی محققات آن را تحت پوشش قرار نمی دهند
و ما از آن گاهی اطلاع نداریم.
که می توانیم پر‌آمدهایی زندگی داشته باشد.


در ویسکانسین یک متهم به شش سال زندان 
محکوم شد
برای فرار از پلیس.
شما ممکنه این را ندانید،
اما الگوریتم به طور افزاینده‌ای در آزادی
مشروط و صدور حکم در حال استفاده هستند
او می خواهد بداند:
چگونه این نمره محاسبه می شود؟
این یک جعبه سیاه تجاری است
شرکت درخواست اینکه الگوریتم 
در دادگاه به چالش کشیده بشود را رد کرد.
اما پروپابلیکا، یک موسسه تحقیقاتی
غیرانتفاعی خیلی از الگوریتم ها را
با اطلاعات عمومی ای که آن ها می‌توانند 
پیدا کنند بررسی می‌کنند.
و دریافتند که این یک نتیجه از تمایلات بوده
و این قدرت پیش‌بینی اشتباه، 
نه فقظ شانسی( بلکه به عمد)
و به طور اشتباه متهمان سیاه را به عنوان
مجرمان آینده دوبرابر نرخ مجرمان سفید
برچست گذاری کرده بود.


خب، به این مورد دقت کنید:
این خانم برای برداشتن 
دخترخوانده‌اش دیر رسید
از یک مدرسه در بروارد ایالت فلوریدا،
با دوستش از خیابان می دوید.
آن ها یک دوچرخه بچه و یک اسکوتر
روی ایوان که قفل نشده بود را نشان دادند
و احمقانه روی آن پرید
وقتی آنها در حال سرعت گرفتن بودند
یک زن آمد و گفت
هی! این دوچرخه بچه من است
آن ها دوچرخه را رها کردن و دور شدند
ولی آن ها دستگیر شدند


او اشتباه کرد و او احمق بود 
ولی او تنها ۱۸ سال داشت
او یک تعدادی جرم‌های کوچک داشت.
ضمنا، آن مرد برای سرقت از فروشگاه
هوم دیپو دستگیر شده بود—
۸۵ دلار، که ارزشش به اندازه جرم کوچک بود.
اما او محکومیت دو سرقت مسلحانه داشت.
اما الگوریتم احتمال بالای جرم برای این زن
نشان می‌داد، و نه برای این مرد.
دو سال بعد، پروپابلیکا یافت
که این نباید در حبس باشد.
و با سایقه‌ای را که داشت
برای او پیدا کردن شغل مشگل بود.
از طرف دیگر این مرد زندانی شد
و برای گناه گذشته‌اش 
برای هشت سال زندانی خواهد بود.
روشن است، ما نیاز داریم تا 
جعبه سیاهمان را بازبینی و بررسی کنیم
و نه آن ها را، بلکه این نوع 
قدرت چک نشده را حسابرسی کنیم


(تشویق حضار)


بررسی و باربینی خوب و مهم است 
اما آنها تمام مشکلات ما را حل نمی کنند.
الگوریتم قدرت خبری فیسبوک
را در نظر بگیرید.
می‌دانید، کسی که همه چیز را رتبه بندی می‌کند
و تصمیم میگیرد که چه به شما نشان دهد
از همه دوستان و همه 
صفحه‌هایی که شما دنبال می‌کنید.
باید به شما عکس بچه‌ی دیگه را نشان دهد؟


(خنده)


یک یادداشت عبوس از یک آشنا؟
یک خبر مهم اما قسمت‌های سختش؟
هیچ جواب درستی وجود ندارد.
فیس بوک برای مشغولیت بیشتر
در سایت بهینه شده :
لایک، اشتراگ گذاری، کامنت


در آگوست ۲۰۱۴
در شهر فرگوست ایالت میسوری معترضان
بعد از کشتن یک نوجوان آفریقایی- آمریکایی
به وسیله یک پلیس سفید پوست
زیر رویداد مبهم شورش کردند،
خبرهای معترضان در همه جا بود
الگورتیم من فید‌های تویتر را فیلتر نکرد
اما در فیس بوکم هیچ جا باز نبود.
آیا اینها دوستان فیسبوکی من بودند؟
من الگوریتم فیسبوکم را
غیر فعال کردم،
که سخت بود زیرا فیسبوک
خواسته های شما را حفظ میکند
تا شما را زیر کنترل الگوریتم ها نگه دارد،
و می بینید که دوستان من در حال 
صحبت کردن درباره این حادثه بودند.
این فقط الگوریتمی بود که این را
به من نشان من داد.
من تحقیق کردم و فهمیدم
که این یه مشکل شایع بود


حادثه فرگوست یک الگوریتم دوستانه نبود.
دوست داشتنی نبود
چه کسی می خواد تا
روی "like" کلیک کنید؟
این حتی ساده نیست 
تا کامنتی روی آن قرار دهید.
بدون "like" و کامنت
احتمالا الگوریتم برای افراد
کمتری را نشان داده می‌شد،
بنابراین ما نتوانستیم این را ببینیم
در عوض، آن هفته،
الگوریتم فیس بوک این را برجسته کرده بود:
این چالش سطل آب یخ است.
علت ارزش، خالی کردن آب یخ، 
کمک به موسسه خیریه خوب است
اما این الگوریتم دوستانه عالی بود.
که ماشین این تصمیم را برای ما گرفت.
و خیلی مهم است اما گفتگوی سختی است
ممکن است خفه شده باشد.
آیا فیسبوک فقط یک کانال داشت؟


خُب، این سیستم ها 
می‌توانند اشتباه باشند
در راه هایی که شباهت
به سیستم انسانی ندارد.
آیا شما واتسون، سیستم 
ماشین هوشمند آی بی ام
که با انسان مسابقه جوپرتری را داد
را به خاطر دارید؟
واتسون بازیگر خوبی در مسابقه بود.
اما در آخرین جوپرتی، از واتسون پرسیده شد
«آیا بزرگترین فرودگاه که برای یک قهرمان
جنگ جهانی دوم نام گذاری شد"
این دومین فرودگاه بزرگ برای 
مبارزه جنگ جهانی دوم نامگذاری شد.»


(...)


شیکاگو.
دو فرد درست جواب دادند
واتسون در طرف دیگر پاسخ داد «تورنتو»—
برای یک شهر آمریکایی!
سیستم موثر همچنین یک اشتباه کرد
که یک فرد هیچ وقت این اشتباه را
نخواهد کرد، حتی یک کلاس دومی.


ماشین هوشمند ما میتواند شکست بخورد
در جاهایی که نمی تواند
الگوی خطای انسان ها را متناسب کند
در جاهایی که ما انتظار
و آمادگی برای آن نخواهیم داشت
این نکبت بار خواهد بود که کسی
که واجد شرایط هست شفلی را نگیرد،
اما سه برابر آن بدتر اینکه به دلیل خرده رفتارها
در روال عادی زندگی فرد آن شغل را نگیرد.


(خنده حضار)


در ماه می ۲۰۱۰،
یک خرابی کوتاه مدت ناشی 
از یک حلقه فیدبک تقویت شد در وال استریت
در الگوریتم« فروش» وال استریت
یک تریلیون دلار ارزش را
در ۳۶ دقیقه از بین برد.
من حتی نمی‌خوام در مورد
معنی‌های «خطا»
در زمینه سلاح های کشنده خودکار فکر کنم.


خب بله، انسان‌ها همیشه تمایلات را می‌سازند.
تصمیم گیرنده‌ها و دربان‌ها
در دادگاه ها و در خبر ها و در جنگ...
آنها اشتباه می کنند،
اما این دقیقا نکته مورد نظر من است.
ما نمی توانیم از این سوالهای
مشگل فرارکنیم.
ما نمی‌توانیم مسئولیت هایمان
را در قبال ماشین ها نادیده بگیریم.


(تشویق)


هوش مصنوعی نمی‌تواند به ما یک کارت 
«خارج شدن از اخلاق به صورت رایگان» بدهد


دانشمند اطلاعات، فرد بنسون
این را "شستشوی ریاضی" می‌نامد
ما به این تضاد نیاز داریم.
ما نیاز داریم تا یک الگوریتم بد گمانی را 
با بررسی دقیق و موشکافانه رشد دهیم.
ما نیاز داریم تا مطمئن باشیم
که مسولیت الگوریتمی داریم،
حسابرسی و شفافیت معنایی نیاز داریم.
ما نیاز داریم تا قبول کنیم که
آورده های ریاضی و محاسباتی
برای ارزش انباشته و به هم ریخته و امور انسانی
عینیت ندارد.
بلکه ، پیچیدگی امور انسانی به
الگوریتم ها حتما غلبه می‌کند.
بله ما می توانیم و ما باید
از محاسبات استفاده کنیم
تا برای داشتن تصمیمات بهتر
به خودمان کمک کنیم
اما ما اعتراف می‌کنیم 
به مسئولیت اخلاقی و قضاوت
و استفااه از الگورتیم هایی با آن چارچوب
نه به عنوان وسیله ای برای کناره گیری
و واگذاری مسئولیت هایمان
به یک انسان دیگر.


هوش مصنوعی اینجاست.
این بدان معناست که ما باید محکم‌تر
ارزش‌ها و اخلاق انسانی را نگه داریم.


متشکرم


(تشویق)

